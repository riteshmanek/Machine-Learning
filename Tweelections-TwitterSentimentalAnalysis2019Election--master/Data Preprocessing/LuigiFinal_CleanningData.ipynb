{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\shira\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\shira\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "DEBUG: Checking if finalClass() is complete\n",
      "DEBUG: Checking if cleanningData() is complete\n",
      "INFO: Informed scheduler that task   finalClass__99914b932b   has status   PENDING\n",
      "INFO: Informed scheduler that task   cleanningData__99914b932b   has status   DONE\n",
      "INFO: Done scheduling tasks\n",
      "INFO: Running Worker with 1 processes\n",
      "DEBUG: Asking scheduler for work...\n",
      "DEBUG: Pending tasks: 1\n",
      "INFO: [pid 14248] Worker Worker(salt=361907149, workers=1, host=DESKTOP-6837UPH, username=shira, pid=14248) running   finalClass()\n",
      "INFO: [pid 14248] Worker Worker(salt=361907149, workers=1, host=DESKTOP-6837UPH, username=shira, pid=14248) done      finalClass()\n",
      "DEBUG: 1 running tasks, waiting for next task to finish\n",
      "INFO: Informed scheduler that task   finalClass__99914b932b   has status   DONE\n",
      "DEBUG: Asking scheduler for work...\n",
      "DEBUG: Done\n",
      "DEBUG: There are no more tasks to run at this time\n",
      "INFO: Worker Worker(salt=361907149, workers=1, host=DESKTOP-6837UPH, username=shira, pid=14248) was stopped. Shutting down Keep-Alive thread\n",
      "INFO: \n",
      "===== Luigi Execution Summary =====\n",
      "\n",
      "Scheduled 2 tasks of which:\n",
      "* 1 complete ones were encountered:\n",
      "    - 1 cleanningData()\n",
      "* 1 ran successfully:\n",
      "    - 1 finalClass()\n",
      "\n",
      "This progress looks :) because there were no failed tasks or missing dependencies\n",
      "\n",
      "===== Luigi Execution Summary =====\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Luigi Completed\n"
     ]
    }
   ],
   "source": [
    "#Working\n",
    "import luigi\n",
    "from boto.s3.connection import S3Connection\n",
    "from boto.s3.key import Key\n",
    "import re\n",
    "import string \n",
    "string.punctuation\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stopword = nltk.corpus.stopwords.words('english')\n",
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "from zipfile import ZipFile\n",
    "#import fastText\n",
    "import sys\n",
    "import os\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import csv\n",
    "import mysql.connector\n",
    "\n",
    "\n",
    "\n",
    "#Creating first task to access code\n",
    "class accessfile(luigi.Task):\n",
    "    \n",
    "    def run(self):\n",
    "        df = pd.read_csv('merged-dataset-week1.csv') \n",
    "        #print(df)\n",
    "        df.to_csv(self.output().path,index = False)\n",
    "        \n",
    "        print(\"Task1 completed\") \n",
    "    \n",
    "    def output(self):\n",
    "        return luigi.LocalTarget(\"merged-dataset-week1_passing.csv\")\n",
    "\n",
    "    \n",
    "#Creating task 2 for cleaning code     \n",
    "class cleanningData(luigi.Task):\n",
    "         \n",
    "    def requires(self):\n",
    "        yield accessfile() \n",
    "        \n",
    "    def run(self):\n",
    "        def lower_tweets(tweets):\n",
    "            lowertweetlist = []\n",
    "            for tweet in tweets:\n",
    "                lowertweetlist.append(tweet.lower())\n",
    "            return lowertweetlist\n",
    "        print(\"run cleaning \")\n",
    "        df = pd.read_csv(accessfile().output().path)\n",
    "        df = df.dropna()\n",
    "        search_congress = ['congress', 'gandhi', 'sonia','rahul', 'Congress','#Congress']\n",
    "        search_bjp = ['modi', 'BJP', 'narendramodi', 'namo', 'bjp', '#BJP']\n",
    "        search_both = ['congress', 'bjp']\n",
    "        search_anti = ['rahulvsmodi', 'congressvsbjp','bjpvscongress']\n",
    "        party=[]\n",
    "        for tweet in df['tweet_text'].str.lower():\n",
    "            if all(x in tweet for x in search_both):\n",
    "                party.append('Both')\n",
    "            elif all(x in tweet for x in search_anti):\n",
    "                party.append('Anti')\n",
    "            elif any(x in tweet for x in search_congress):\n",
    "                party.append('Congress')\n",
    "            elif any(x in tweet for x in search_bjp):\n",
    "                party.append('BJP')\n",
    "            else:\n",
    "                party.append('Others')\n",
    "        df['party'] = party\n",
    "        print(\"before method\")\n",
    "\n",
    "        tweets = df['tweet_text']\n",
    "        lowertweetlist = lower_tweets(tweets)    \n",
    "        #lowertweetlist = lower_tweets(tweets)\n",
    "        def clean_tweets(tweets):\n",
    "            processed_tweets = [] \n",
    "            for tweet in tweets:\n",
    "                tweet = re.sub(r'((www\\.[\\S]+)|(https?://[\\S]+))', '', tweet) #Remove URLs\n",
    "                tweet = re.sub(r'\\\\x[a-zA-Z0-9][a-zA-Z0-9]',r'',tweet) #Remove special texts, eg: \\xe8\n",
    "                tweet = re.sub(r'@[\\S]+', '', tweet) #Remove @user mentions\n",
    "                tweet = re.sub(r'\\.{2,}', ' ', tweet) #Remove more than 2 dots with a space\n",
    "                tweet = re.sub(r'\\,', ' ', tweet) #Replace ,  with space\n",
    "                tweet = tweet.strip(' \"\\'') #Strip space, \" and ' from tweet\n",
    "                tweet = re.sub(r'\\s+', ' ', tweet) #Replace multiple spaces with a single space\n",
    "                tweet = re.sub(r'[b][\\'\"]', '', tweet)\n",
    "                tweet = re.sub(r'\\brt\\b', '', tweet)\n",
    "                processed_tweets.append(tweet)\n",
    "            return processed_tweets\n",
    "        tweets = df['tweet_text']\n",
    "        df['tweet_text'] = clean_tweets(tweets)\n",
    "        #special character \n",
    "        bad_chars = ['à', '¤', '¶', \"•\", \"à\", \"´\",\"¨\",\"±\",\"µ\",\"Ÿ\",\"à\",\"“\",\"¦\",\"¬\",\"š\"]\n",
    "        processed_tweet =[]\n",
    "        def badCharRemove(tweets):\n",
    "            for tweet in df['tweet_text']:\n",
    "                    tweet = re.sub(\"|\".join(bad_chars), \" \",tweet)\n",
    "                    processed_tweet.append(tweet)\n",
    "            return processed_tweet\n",
    "        \n",
    "        tweets = df['tweet_text']\n",
    "        df['tweet_text'] = badCharRemove(tweets)\n",
    "        \n",
    "        # remove non-ASCII-special character \n",
    "        def remove_non_ascii(tweet_text):\n",
    "            return ''.join(i for i in tweet_text if ord(i)<128)\n",
    "        df['tweet_text'] = df['tweet_text'].apply(remove_non_ascii)\n",
    "        \n",
    "        \n",
    "        def replace_emojis(tweets):\n",
    "            replace_emojis = [] \n",
    "            for tweet in tweets:\n",
    "        # Smile -- :), : ), :-), (:, ( :, (-:, :')\n",
    "                tweet = re.sub(r'(:\\s?\\)|:-\\)|\\(\\s?:|\\(-:|:\\'\\))', ' Smile ', tweet)\n",
    "        # Laugh -- :D, : D, :-D, xD, x-D, XD, X-D\n",
    "                tweet = re.sub(r'(:\\s?D|:-D|x-?D|X-?D)', ' Laugh ', tweet)\n",
    "        # Love -- <3, :*\n",
    "                tweet = re.sub(r'(<3|:\\*)', ' Love ', tweet)\n",
    "        # Wink -- ;-), ;), ;-D, ;D, (;,  (-;\n",
    "                tweet = re.sub(r'(;-?\\)|;-?D|\\(-?;)', ' Wink ', tweet)\n",
    "        # Sad -- :-(, : (, :(, ):, )-:\n",
    "                tweet = re.sub(r'(:\\s?\\(|:-\\(|\\)\\s?:|\\)-:)', ' Sad ', tweet)\n",
    "        # Cry -- :,(, :'(, :\"(\n",
    "                tweet = re.sub(r'(:,\\(|:\\'\\(|:\"\\()', ' Cry ', tweet)\n",
    "                replace_emojis.append(tweet)\n",
    "            return replace_emojis\n",
    "        \n",
    "        tweets = df['tweet_text']\n",
    "        df['tweet_text'] = replace_emojis(tweets)\n",
    "        \n",
    "        def remove_punct(text):\n",
    "            text  = \"\".join([char for char in text if char not in string.punctuation])\n",
    "            text = re.sub('[0-9]+', '', text)\n",
    "            return text\n",
    "\n",
    "        df['tweet_text'] = df['tweet_text'].apply(lambda x: remove_punct(x))\n",
    "        \n",
    "        def tokenization(text):\n",
    "            text = re.split('\\W+', text)\n",
    "            return text\n",
    "\n",
    "        df['tweet_text'] = df['tweet_text'].apply(lambda x: tokenization(x))\n",
    "        \n",
    "        def remove_stopwords(text):\n",
    "            text = [word for word in text if word not in stopword]\n",
    "            return text\n",
    "    \n",
    "        df['tweet_text'] = df['tweet_text'].apply(lambda x: remove_stopwords(x))\n",
    "        \n",
    "        ps = nltk.PorterStemmer()\n",
    "\n",
    "        def stemming(text):\n",
    "            text = [ps.stem(word) for word in text]\n",
    "            return text\n",
    "\n",
    "        df['tweet_text'] = df['tweet_text'].apply(lambda x: stemming(x))\n",
    "        \n",
    "        wn = nltk.WordNetLemmatizer()\n",
    "\n",
    "        def lemmatizer(text):\n",
    "            text = [wn.lemmatize(word) for word in text]\n",
    "            return text\n",
    "\n",
    "        df['tweet_text'] = df['tweet_text'].apply(lambda x: lemmatizer(x))\n",
    "        def rejoin_words(row):\n",
    "            my_list = row['tweet_text']\n",
    "            joined_words = (\" \".join(my_list))\n",
    "            return joined_words\n",
    "\n",
    "        df['tweet_text'] = df.apply(rejoin_words, axis=1)\n",
    "        df[\"is_duplicate\"]= df['tweet_text'].duplicated()\n",
    "        df = df.drop(df.index[df['is_duplicate'] == True])\n",
    "        \n",
    "        #df.to_csv('labelled-train-Luigi.csv', encoding='utf-8', index=False)\n",
    "        \n",
    "        df.to_csv(self.output().path,index = False)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #df.to_csv(\"LuigiDataTry1.csv\")\n",
    "    \n",
    "        \n",
    "        \n",
    "        print(\"task2 completed \") \n",
    "    def output(self):\n",
    "        return luigi.LocalTarget(\"merged-dataset-week1_Luigi.csv\")\n",
    "\n",
    "\n",
    "#Creating Task for pushing code to RDS    \n",
    "class finalClass(luigi.Task):\n",
    "    def requires(self):\n",
    "        yield cleanningData()\n",
    "        \n",
    "    def run(self):\n",
    "        df = pd.read_csv(cleanningData().output().path)\n",
    "        print(\"Luigi Completed\")\n",
    "        \n",
    "   \n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    luigi.build([finalClass()], local_scheduler=True)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
